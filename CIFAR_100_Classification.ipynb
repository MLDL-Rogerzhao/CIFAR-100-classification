{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Some preprocessing on the device and importing the required libraries.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf  \n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar100.load_data(label_mode='fine')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator # Data Augmentation\n",
    "from keras.layers import BatchNormalization # Has a very good result and in the speed\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten,Dropout\n",
    "from keras.utils import normalize, to_categorical\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force memory can only be used by the GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1: Data acquistion and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## *1. Labelling all the catergories for the dataset to better visualization after*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache path for CIFAR-100 dataset\n",
    "cifar100_path = os.path.expanduser('~/.keras/datasets/cifar-100-python')\n",
    "\n",
    "# Loading metadata files\n",
    "def load_cifar100_meta(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        meta = pickle.load(f, encoding='bytes')\n",
    "    return meta\n",
    "\n",
    "# Extract fine-grained category names\n",
    "meta = load_cifar100_meta(os.path.join(cifar100_path, 'meta'))\n",
    "fine_label_names = [label.decode('utf-8') for label in meta[b'fine_label_names']]\n",
    "\n",
    "# Print the category name to confirm\n",
    "print(\"CIFAR-100 Fine Label Names:\")\n",
    "print(fine_label_names)\n",
    "print(f\"Total number of fine labels: {len(fine_label_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = [\n",
    "    'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', 'bottle',\n",
    "    'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 'can', 'castle', 'caterpillar', 'cattle',\n",
    "    'chair', 'chimpanzee', 'clock', 'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur',\n",
    "    'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 'house', 'kangaroo', 'keyboard',\n",
    "    'lamp', 'lawn_mower', 'leopard', 'lion', 'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain',\n",
    "    'mouse', 'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear', 'pickup_truck', 'pine_tree',\n",
    "    'plain', 'plate', 'poppy', 'porcupine', 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket',\n",
    "    'rose', 'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider',\n",
    "    'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', 'tank', 'telephone', 'television', 'tiger', 'tractor',\n",
    "    'train', 'trout', 'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm'\n",
    "]\n",
    "len(label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## *2. Using One-Hot Encoding on y_train and y_test*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "print('The y_train shape has been changed to:',y_train.shape)\n",
    "print('The y_train shape has been changed to:',y_test.shape)\n",
    "\n",
    "# The integer labels are converted into a format suitable for the multiclassification task,\n",
    "# but care needs to be taken with the continuity of the labels and the choice of the loss function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The integer labels are converted into a format suitable for the multiclassification task, but care needs to be taken with the continuity of the labels and the choice of the loss function. Then we will choose the softmax as output activations functions and categorical_crossentropy as loss functions*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *3.Normalizing the x_train and x_test changing the pixel values from 0-255 into 0-1*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = normalize(x_train,axis=1)\n",
    "x_test  = normalize(x_test,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *4. Spliting the training set into validation set*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_split, x_val, y_train_split, y_val = train_test_split(\n",
    "    x_train, y_train,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=np.argmax(y_train, axis=1)  # ensure all the catergories distributes the same when running the code each times.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *5. Data Augmentation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='reflect'\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow(x_train_split,y_train_split,batch_size=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing all the dataset shapes\n",
    "print(\"Dataset Shapes\".center(50, '-'))\n",
    "print(f\"{'Training set x shape:':<25} {x_train.shape}\")\n",
    "print(f\"{'Validation set x shape:':<25} {x_val.shape}\")\n",
    "print(f\"{'Testing set x shape:':<25} {x_test.shape}\")\n",
    "print(f\"{'Training set y shape:':<25} {y_train.shape}\")\n",
    "print(f\"{'Validation set y shape:':<25} {y_val.shape}\")\n",
    "print(f\"{'Testing set y shape:':<25} {y_test.shape}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Checking the values of y_train and x_train beforehand to check* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_train[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D,BatchNormalization,Dropout,GlobalAveragePooling2D \n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_HEIGHT= x_train.shape[1]\n",
    "IMAGE_WIDTH = x_train.shape[2]\n",
    "IMAGE_CHANNELS= x_train.shape[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2: Building a simple CNN model first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This is an initial version of CNN model that I built, it is used to gain a basic understanding of an overview framework for this Multi-classification task as an initial experiment.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Some very vital model parameters definition which help better understand the model*\n",
    "- *Loss function (categorial crossentropy) : Quantifies the error between output of the algorithm and given target value, in order to find the golabl minimum value by the optimizer shown here as adam.*\n",
    "\n",
    "- *evaluated metrics: accuracy defined by the corrected number rate.*\n",
    "\n",
    "- *Optimizers update the model in response to the output of the loss function.*\n",
    "\n",
    "- *activation functions: importing non-linear factor to the results*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The initial CNN model structures: \n",
    "# activation = 'relu'\n",
    "# model = Sequential()\n",
    "# model.add(Conv2D(32, 3, activation = activation, padding = 'same', input_shape = (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS)))\n",
    "# model.add(BatchNormalization())\n",
    "\n",
    "# model.add(Conv2D(32, 3, activation = activation, padding = 'same', kernel_initializer = 'he_uniform'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(MaxPooling2D())\n",
    "\n",
    "# model.add(Conv2D(64, 3, activation = activation, padding = 'same', kernel_initializer = 'he_uniform'))\n",
    "# model.add(BatchNormalization())\n",
    "\n",
    "# model.add(Conv2D(64, 3, activation = activation, padding = 'same', kernel_initializer = 'he_uniform'))\n",
    "# model.add(BatchNormalization()) \n",
    "# model.add(MaxPooling2D())\n",
    "\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(256, activation = activation, kernel_initializer = 'he_uniform'))\n",
    "# model.add(Dense(100, activation = 'softmax'))\n",
    "\n",
    "# model.compile(optimizer = 'adam',loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "# print(model.summary()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks = [\n",
    "#     tf.keras.callbacks.ModelCheckpoint(\n",
    "#         filepath=r\"C:\\vscodeproject\\Python\\EE992\\model_weight\\model1_augmented_weight.h5\", \n",
    "#         save_best_only=True, \n",
    "#         verbose=1,\n",
    "#         monitor='val_loss', \n",
    "#         mode='min'           \n",
    "#     )]\n",
    "\n",
    " \n",
    "# history1 = model.fit_generator(\n",
    "#     train_generator,\n",
    "#     steps_per_epoch=1000,\n",
    "#     epochs=25,\n",
    "#     validation_data=(x_val, y_val),\n",
    "#     callbacks=callbacks,  \n",
    "#     verbose=1,\n",
    "#     shuffle=False\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building more complex CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = 'relu'\n",
    "model2 = Sequential()\n",
    "model2.add(Conv2D(32, 3, activation = activation, padding = 'same', input_shape = (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS)))\n",
    "model2.add(BatchNormalization())\n",
    "\n",
    "model2.add(Conv2D(32, 3, activation = activation, padding = 'same', kernel_initializer = 'he_uniform'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(MaxPooling2D())\n",
    "\n",
    "model2.add(Conv2D(64, 3, activation = activation, padding = 'same', kernel_initializer = 'he_uniform'))\n",
    "model2.add(BatchNormalization())\n",
    "\n",
    "model2.add(Conv2D(64, 3, activation = activation, padding = 'same', kernel_initializer = 'he_uniform'))\n",
    "model2.add(BatchNormalization()) \n",
    "model2.add(MaxPooling2D())\n",
    "\n",
    "model2.add(Conv2D(128, 3, activation = activation, padding = 'same', kernel_initializer = 'he_uniform'))\n",
    "model2.add(BatchNormalization())\n",
    "\n",
    "model2.add(Conv2D(128, 3, activation = activation, padding = 'same', kernel_initializer = 'he_uniform'))\n",
    "model2.add(BatchNormalization()) \n",
    "model2.add(MaxPooling2D())\n",
    "\n",
    "model2.add(Conv2D(256, 3, activation=activation, padding='same', kernel_initializer='he_uniform'))\n",
    "model2.add(BatchNormalization())\n",
    "\n",
    "model2.add(Conv2D(256, 3, activation=activation, padding='same', kernel_initializer='he_uniform'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(MaxPooling2D())\n",
    "\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(1024, activation = activation, kernel_initializer = 'he_uniform'))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(100, activation = 'softmax'))\n",
    "\n",
    "model2.compile(optimizer = 'adam',loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "print(model2.summary()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=r\"C:\\vscodeproject\\Python\\EE992\\model_weight\\model2_augmented_best_val_acc.h5\", \n",
    "        save_best_only=True, \n",
    "        verbose=1,\n",
    "        monitor='val_accuracy', \n",
    "        mode='max'           \n",
    "    )]\n",
    "\n",
    " \n",
    "history2 = model2.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=1000,\n",
    "    epochs=50,\n",
    "    validation_data=(x_val, y_val),\n",
    "    callbacks=callbacks,  \n",
    "    verbose=1,\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.applications import ResNet50\n",
    "# from tensorflow.keras.models import Model\n",
    "\n",
    "# base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
    "# x = base_model.output\n",
    "# x = GlobalAveragePooling2D()(x)\n",
    "# x = Dense(256, activation='relu')(x)\n",
    "# x = Dropout(0.2)(x)\n",
    "# predictions = Dense(100, activation='softmax')(x)\n",
    "# model3 = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# for layer in base_model.layers:\n",
    "#     layer.trainable = False\n",
    "\n",
    "# model3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# print(model3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks = [\n",
    "#     tf.keras.callbacks.ModelCheckpoint(\n",
    "#         filepath=r\"C:\\vscodeproject\\Python\\EE992\\model_weight\\model3_augmented_best_val_loss_transfer_learning.h5\", \n",
    "#         save_best_only=True, \n",
    "#         verbose=1,\n",
    "#         monitor='val_loss', \n",
    "#         mode='min'           \n",
    "#     )]\n",
    "\n",
    " \n",
    "# history3 = model3.fit_generator(\n",
    "#     train_generator,\n",
    "#     steps_per_epoch=1000,\n",
    "#     epochs=50,\n",
    "#     validation_data=(x_val, y_val),\n",
    "#     callbacks=callbacks,  \n",
    "#     verbose=1,\n",
    "#     shuffle=False\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3 : Visualisation of the validation and training sets of the loss and accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *plot the training and validation accuracy and loss at each epoch*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history2.history['loss']\n",
    "val_loss = history2.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*(These are just the logbook contents record my progressed processing based on the plot. Ticket means what I have done)*\n",
    "\n",
    "- *Adding more layers*\n",
    "\n",
    "- *Regularisation(dropout etc)*\n",
    "\n",
    "- *Data augmentation âœ”*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "acc = history2.history['accuracy']\n",
    "val_acc = history2.history['val_accuracy']\n",
    "\n",
    "plt.plot(epochs, acc, 'y', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Evaluate the training model on testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model2.predict(x_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1) # Using argmax can return the index of the one-hot vector so that it can tell the certain type of the prediction class\n",
    "y_true = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The shape of y_pred is:', y_pred_classes.shape)\n",
    "print(f'The shape of y_true is:', y_true.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the testing sample\n",
    "num_samples = 10 # It is adjustable, can be changed to different number from 0-10000.\n",
    "for i in range(num_samples):\n",
    "    true_label = fine_label_names[y_true[i]]  # The true label name\n",
    "    pred_label = fine_label_names[y_pred_classes[i]]  # The predicted type name\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"  True Label: {true_label}\")\n",
    "    print(f\"  Predicted Label: {pred_label}\")\n",
    "    print(f\"  Correct: {true_label == pred_label}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Visualization the images and categories*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation of the images and categories for the first 10 images\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i in range(num_samples):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(x_test[i])  # Showing the x_test data images\n",
    "    true_label = fine_label_names[y_true[i]] # showing the true label catergories\n",
    "    pred_label = fine_label_names[y_pred_classes[i]] # showing the prediction catergories\n",
    "    plt.title(f\"True: {true_label}\\nPred: {pred_label}\", fontsize=10)\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Script(record the operations that I have tried):\n",
    "- 1. After adding data augmentation the correct item improved from 1 to 2.\n",
    "- 2. After optimizing the data augmentation hyperparameters such as the rotation_range from 45% to 15%, the correct itemes improved from 2 to 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Step5: Confusion matrix visualisation and classification report*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred_classes)\n",
    "\n",
    "# Find the top 10 catergories for better visualization the 100 X 100 confusion matrix otherwise it will be too dense to see the matrix.\n",
    "top_k = 10\n",
    "# Calculate the total number of predictions for each category (off-diagonal sum)\n",
    "pred_counts = cm.sum(axis=0)  #  The sum of each column, indicating the total number projected for the category\n",
    "top_k_indices = np.argsort(pred_counts)[-top_k:]  # Index of the top 10 most predicted categories\n",
    "\n",
    "# Extracting the sub confusion matrix\n",
    "sub_cm = cm[np.ix_(top_k_indices, top_k_indices)]\n",
    "sub_label_names = [label_names[i] for i in top_k_indices]\n",
    "\n",
    "# Visualiza the sub confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(sub_cm, annot=True, fmt='d', cmap='Blues', xticklabels=sub_label_names, yticklabels=sub_label_names)\n",
    "plt.title('Confusion Matrix (Top 10 Predicted Classes)')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The classification report\n",
    "report = classification_report(y_true, y_pred_classes, target_names=label_names)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model2.evaluate(x_test, y_test, verbose=1)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Unet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
